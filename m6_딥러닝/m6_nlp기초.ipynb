{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyN3Yj6KDbL2NtnAPNLoQoHJ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["### 텍스트의 토큰화\n","- NLP(자연어 처리) 토큰화는 텍스트 분석, 언어 번역, 감정 분석 등과 같은 NLP 작업을 위해 텍스트를 준비하는 프로세스의 기본 단계.\n","- 토큰화에는 텍스트를 단어, 문구, 기호 또는 토큰이라고 하는 기타 의미 있는 요소와 같은 개별 구성 요소로 분해하는 작업이 포함. 토큰은 추가 처리 및 분석을 위한 구성 요소가 된다.\n","- 토큰화의 목적은 텍스트 데이터를 단순화하고 알고리즘이 이해하고 처리할 수 있도록 관리하기 쉽게 만드는 것이다. NLP 작업의 특정 요구 사항에 따라 토큰은 단어, 문장 또는 단어의 일부(예: 어간 또는 하위 단어)일 수도 있다. 이 프로세스는 불필요한 구두점과 공백을 제거하는 데 도움이 되며 데이터 세트 전체에서 일관성을 유지하기 위해 모든 텍스트를 소문자로 변환하는 작업도 포함될 수 있다."],"metadata":{"id":"RLwPRC1zwFeO"}},{"cell_type":"code","execution_count":33,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d3Ra2nJlwAyy","executionInfo":{"status":"ok","timestamp":1707194029581,"user_tz":-540,"elapsed":328,"user":{"displayName":"이민지","userId":"02649806100014329289"}},"outputId":"29cc42a3-0484-4d7a-d75d-5708885aa750"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","원문:\n"," 해보지 않으면 해낼 수 없다\n","\n","토큰화:\n"," ['해보지', '않으면', '해낼', '수', '없다']\n"]}],"source":["from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense,Flatten,Embedding\n","from tensorflow.keras.utils import to_categorical\n","from numpy import array\n","\n","# 케라스의 텍스트 전처리와 관련된 함수중 text_to_word_sequence 함수를 불러옵니다.\n","from tensorflow.keras.preprocessing.text import text_to_word_sequence\n","\n","# 전처리할 텍스트를 정합니다.\n","text = '해보지 않으면 해낼 수 없다'\n","\n","# 해당 텍스트를 토큰화합니다.\n","result = text_to_word_sequence(text)\n","print('\\n원문:\\n', text)\n","print('\\n토큰화:\\n', result)"]},{"cell_type":"markdown","source":["- Tokenizer' 클래스는 텍스트를 정수 시퀀스로 변환하도록 설계\n","- fit_on_texts(docs): 이 메소드는 문장 목록(docs)을 인수로 사용하여 token 개체에서 호출된다. 텍스트 목록을 기반으로 내부 어휘를 업데이트하여 토크나이저가 이러한 텍스트로 작업할 수 있도록 준비한다. 말뭉치의 각 고유 단어에 색인을 할당하고 단어 빈도와 같은 다양한 측정항목을 계산하는 작업이 포함된다.\n","- token.word_counts: 토크나이저를 텍스트에 맞춘 후 word_counts는 키가 입력 텍스트에서 발견된 단어이고 값은 각 단어의 발생 횟수인 OrderedDict를 제공한다. 'OrderedDict'를 사용하면 단어가 텍스트에서 처음 나타나는 순서대로 정렬.\n","- token.document_count: 이 속성은 처리된 총 문서(또는 문장) 수를 표시\n","- token.word_docs: word_counts와 유사한 OrderedDict이지만 단어의 빈도 대신 각 단어가 나타나는 문서 수를 표시\n","- token.word_index: 이 속성은 단어를 고유하게 할당된 정수에 매핑하는 OrderedDict를 제공. 모델에는 숫자 입력이 필요하므로 이는 기계 학습 모델의 텍스트를 벡터화하는 데 필요"],"metadata":{"id":"_UsZoPHDzTBw"}},{"cell_type":"code","source":["# 단어 빈도수 세기\n","\n","# 전처리하려는 세 개의 문장을 정합니다.\n","docs = ['먼저 텍스트의 각 단어를 나누어 토큰화 합니다.',\n","        '텍스트의 단어로 토큰화해야 딥러닝에서 인식됩니다.',\n","        '토큰화한 결과는 딥러닝에서 사용할 수 있습니다.',\n","        ]\n","\n","# 토큰화 함수를 이용해 전처리 하는 과정입니다.\n","token = Tokenizer()               # 토큰화 함수 지정\n","token.fit_on_texts(docs)          # 토큰화 함수에 문자 적용\n","\n","# 단어의 빈도수를 계산한 결과를 각 옵션에 맞추어 출력합니다.\n","# Tokenizer()의 word_counts 함수는 순서를 기억하는 OrderdDict 클래스를 사용합니다.\n","print('\\n단어 카운트:\\n', token.word_counts)\n","print('\\n문장 카운트:\\n', token.document_count)\n","print('\\n각 단어가 몇 개의 문장에 포함되어 있는가:\\n', token.word_docs)\n","# token.word_index의 출력 순서는 제공된 텍스트 코퍼스의 각 단어의 빈도에 따라 가장 빈번한 단어부터 가장 빈도가 낮은 단어까지 결정\n","print('\\n각 단어에 매겨진 인덱스 값:\\n', token.word_index)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"plGSfZmyyzGQ","executionInfo":{"status":"ok","timestamp":1707194030934,"user_tz":-540,"elapsed":2,"user":{"displayName":"이민지","userId":"02649806100014329289"}},"outputId":"cb0136db-9667-40d2-f60b-d0ca30a47c51"},"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","단어 카운트:\n"," OrderedDict([('먼저', 1), ('텍스트의', 2), ('각', 1), ('단어를', 1), ('나누어', 1), ('토큰화', 1), ('합니다', 1), ('단어로', 1), ('토큰화해야', 1), ('딥러닝에서', 2), ('인식됩니다', 1), ('토큰화한', 1), ('결과는', 1), ('사용할', 1), ('수', 1), ('있습니다', 1)])\n","\n","문장 카운트:\n"," 3\n","\n","각 단어가 몇 개의 문장에 포함되어 있는가:\n"," defaultdict(<class 'int'>, {'텍스트의': 2, '나누어': 1, '합니다': 1, '토큰화': 1, '각': 1, '먼저': 1, '단어를': 1, '토큰화해야': 1, '인식됩니다': 1, '단어로': 1, '딥러닝에서': 2, '결과는': 1, '토큰화한': 1, '수': 1, '있습니다': 1, '사용할': 1})\n","\n","각 단어에 매겨진 인덱스 값:\n"," {'텍스트의': 1, '딥러닝에서': 2, '먼저': 3, '각': 4, '단어를': 5, '나누어': 6, '토큰화': 7, '합니다': 8, '단어로': 9, '토큰화해야': 10, '인식됩니다': 11, '토큰화한': 12, '결과는': 13, '사용할': 14, '수': 15, '있습니다': 16}\n"]}]},{"cell_type":"markdown","source":["Q. 주어진 docs를 토큰화해서 아래사항을 수행하세요."],"metadata":{"id":"qi1ti_KM2_Co"}},{"cell_type":"code","source":["docs = ['검찰이 제시한 혐의 사실 전부를 재판부가 무죄로 판단하면서 이 회장은 검찰 기소 이후 3년 5개월여 만에 시름을 덜게 됐다.',\n","'검찰 항소로 2심 재판이 진행될 것이란 전망이 나오지만,']\n","\n","token = Tokenizer()\n","token.fit_on_texts(docs)\n","\n","\n","print('\\n단어 카운트:\\n', token.word_counts)\n","print('\\n문장 카운트:\\n', token.document_count)\n","print('\\n각 단어가 몇 개의 문장에 포함되어 있는가:\\n', token.word_docs)\n","print('\\n각 단어에 매겨진 인덱스 값:\\n', token.word_index)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4g4ChidD2IS9","executionInfo":{"status":"ok","timestamp":1707194032719,"user_tz":-540,"elapsed":1,"user":{"displayName":"이민지","userId":"02649806100014329289"}},"outputId":"849d51f8-bc7a-4242-85f2-d4fc6d4d7b94"},"execution_count":35,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","단어 카운트:\n"," OrderedDict([('검찰이', 1), ('제시한', 1), ('혐의', 1), ('사실', 1), ('전부를', 1), ('재판부가', 1), ('무죄로', 1), ('판단하면서', 1), ('이', 1), ('회장은', 1), ('검찰', 2), ('기소', 1), ('이후', 1), ('3년', 1), ('5개월여', 1), ('만에', 1), ('시름을', 1), ('덜게', 1), ('됐다', 1), ('항소로', 1), ('2심', 1), ('재판이', 1), ('진행될', 1), ('것이란', 1), ('전망이', 1), ('나오지만', 1)])\n","\n","문장 카운트:\n"," 2\n","\n","각 단어가 몇 개의 문장에 포함되어 있는가:\n"," defaultdict(<class 'int'>, {'무죄로': 1, '덜게': 1, '이': 1, '사실': 1, '전부를': 1, '됐다': 1, '3년': 1, '판단하면서': 1, '혐의': 1, '5개월여': 1, '만에': 1, '회장은': 1, '이후': 1, '검찰이': 1, '재판부가': 1, '시름을': 1, '제시한': 1, '검찰': 2, '기소': 1, '항소로': 1, '진행될': 1, '전망이': 1, '것이란': 1, '재판이': 1, '나오지만': 1, '2심': 1})\n","\n","각 단어에 매겨진 인덱스 값:\n"," {'검찰': 1, '검찰이': 2, '제시한': 3, '혐의': 4, '사실': 5, '전부를': 6, '재판부가': 7, '무죄로': 8, '판단하면서': 9, '이': 10, '회장은': 11, '기소': 12, '이후': 13, '3년': 14, '5개월여': 15, '만에': 16, '시름을': 17, '덜게': 18, '됐다': 19, '항소로': 20, '2심': 21, '재판이': 22, '진행될': 23, '것이란': 24, '전망이': 25, '나오지만': 26}\n"]}]},{"cell_type":"markdown","source":["### 단어의 원-핫 인코딩\n","- word_size = len(token.word_index) + 1: 여기서 token.word_index는 키가 단어이고 값이 해당 고유 정수 인덱스인 사전. token.word_index의 길이는 말뭉치에 있는 고유 단어의 총 개수를 나타낸다. 이 개수에 1을 추가하는 것은 \"0\" 인덱스를 설명하기 위해 NLP에서 일반적인 관행이며는 종종 패딩에 사용되거나 구현에 따라 알 수 없는 단어를 나타낼 수 있습니다.\n","- x = to_categorical(x, num_classes=word_size): 이 줄은 단어를 나타내는 정수 인덱스의 목록 또는 배열이어야 하는 x를 원-핫 인코딩 형식으로 변환한다. to_categorical은 클래스 벡터(정수)를 바이너리 클래스 행렬로 변환하는 함수(일반적으로 Keras의)이며 x의 각 정수에 대해 to_categorical은 1로 설정된 정수 인덱스에 해당하는 위치를 제외하고 모든 요소가 0인 길이 word_size의 벡터를 생성한다.\n","- num_classes=word_size는 총 클래스 수를 지정한다. 이 경우 어휘 크기(word_size)로 설정되어 원-핫 인코딩에 어휘의 모든 단어에 대한 슬롯과 추가 \"0\" 인덱스가 있는지 확인한다."],"metadata":{"id":"bth5zBbx4L1L"}},{"cell_type":"code","source":["text = '오랫동안 꿈꾸는 이는 그 꿈을 닮아간다'\n","token = Tokenizer()\n","token.fit_on_texts([text])\n","print(token.word_index)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XWSv0SXN4OtN","executionInfo":{"status":"ok","timestamp":1707194034730,"user_tz":-540,"elapsed":1,"user":{"displayName":"이민지","userId":"02649806100014329289"}},"outputId":"4958238f-08cd-4138-ee46-191f75e23a13"},"execution_count":36,"outputs":[{"output_type":"stream","name":"stdout","text":["{'오랫동안': 1, '꿈꾸는': 2, '이는': 3, '그': 4, '꿈을': 5, '닮아간다': 6}\n"]}]},{"cell_type":"code","source":["x=token.texts_to_sequences([text])\n","print(x)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L_AJxYil76X5","executionInfo":{"status":"ok","timestamp":1707194035672,"user_tz":-540,"elapsed":2,"user":{"displayName":"이민지","userId":"02649806100014329289"}},"outputId":"a1b6dd8c-5b61-4075-b693-4674749509c1"},"execution_count":37,"outputs":[{"output_type":"stream","name":"stdout","text":["[[1, 2, 3, 4, 5, 6]]\n"]}]},{"cell_type":"code","source":["# 인덱스 수에 하나를 추가해서 원-핫 인코딩 배열 만들기\n","# word_size 에 1을 추가하는 이유는 Keras의 Tokenizer를 사용할 때 0 인덱스를 패딩(padding)을 위해 예약하는 관례 때문\n","word_size = len(token.word_index) + 1\n","x = to_categorical(x, num_classes=word_size)\n","print(x)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h1J84IDq8Bpy","executionInfo":{"status":"ok","timestamp":1707194036402,"user_tz":-540,"elapsed":2,"user":{"displayName":"이민지","userId":"02649806100014329289"}},"outputId":"9c79408f-0f24-4c72-b642-6754e75a484f"},"execution_count":38,"outputs":[{"output_type":"stream","name":"stdout","text":["[[[0. 1. 0. 0. 0. 0. 0.]\n","  [0. 0. 1. 0. 0. 0. 0.]\n","  [0. 0. 0. 1. 0. 0. 0.]\n","  [0. 0. 0. 0. 1. 0. 0.]\n","  [0. 0. 0. 0. 0. 1. 0.]\n","  [0. 0. 0. 0. 0. 0. 1.]]]\n"]}]},{"cell_type":"markdown","source":["# 텍스트를 읽고 긍정, 부정 예측하기"],"metadata":{"id":"7DVIERVO4L0J"}},{"cell_type":"code","source":["docs = [\"너무 재밌네요\",\"최고예요\",\"참 잘 만든 영화예요\",\"추천하고 싶은 영화입니다\",\"한번 더 보고싶네요\",\"글쎄요\",\"별로예요\",\"생각보다 지루하네요\",\"연기가 어색해요\",\"재미없어요\"]\n","\n","# 긍정 리뷰는 1, 부정 리뷰는 0으로 클래스를 지정합니다.\n","classes = array([1,1,1,1,1,0,0,0,0])\n","\n","# 토큰화\n","token = Tokenizer()\n","token.fit_on_texts(docs)\n","print(token.word_index)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8Q0b7mAw-W3e","executionInfo":{"status":"ok","timestamp":1707194037604,"user_tz":-540,"elapsed":2,"user":{"displayName":"이민지","userId":"02649806100014329289"}},"outputId":"b19a9a63-c27a-4c9b-975c-1353b3724aae"},"execution_count":39,"outputs":[{"output_type":"stream","name":"stdout","text":["{'너무': 1, '재밌네요': 2, '최고예요': 3, '참': 4, '잘': 5, '만든': 6, '영화예요': 7, '추천하고': 8, '싶은': 9, '영화입니다': 10, '한번': 11, '더': 12, '보고싶네요': 13, '글쎄요': 14, '별로예요': 15, '생각보다': 16, '지루하네요': 17, '연기가': 18, '어색해요': 19, '재미없어요': 20}\n"]}]},{"cell_type":"code","source":["x = token.texts_to_sequences(docs)\n","print('\\n리뷰 텍스트, 토큰화 결과:\\n', x)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YROgDu4tBLNI","executionInfo":{"status":"ok","timestamp":1707194039353,"user_tz":-540,"elapsed":683,"user":{"displayName":"이민지","userId":"02649806100014329289"}},"outputId":"10574111-e3a3-42eb-e317-75da8ac7dbf3"},"execution_count":40,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","리뷰 텍스트, 토큰화 결과:\n"," [[1, 2], [3], [4, 5, 6, 7], [8, 9, 10], [11, 12, 13], [14], [15], [16, 17], [18, 19], [20]]\n"]}]},{"cell_type":"code","source":["# 패딩, 서로 다른 길이의 데이터를 4로 맞추어 줍니다.\n","# padding='pre' : 시퀀스의 길이가 4보다 짧을 경우 앞쪽을 0으로 채워 길이를 4로 맞춘다.\n","padded_x = pad_sequences(x, 4, padding='pre') # default\n","# padded_x = pad_sequences(x, 4, padding = 'post')\n","print('\\n패딩 결과:\\n', padded_x)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UnwUGtMjBe9d","executionInfo":{"status":"ok","timestamp":1707194039687,"user_tz":-540,"elapsed":2,"user":{"displayName":"이민지","userId":"02649806100014329289"}},"outputId":"0b61933f-909a-4303-cec8-22e74ebef7a8"},"execution_count":41,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","패딩 결과:\n"," [[ 0  0  1  2]\n"," [ 0  0  0  3]\n"," [ 4  5  6  7]\n"," [ 0  8  9 10]\n"," [ 0 11 12 13]\n"," [ 0  0  0 14]\n"," [ 0  0  0 15]\n"," [ 0  0 16 17]\n"," [ 0  0 18 19]\n"," [ 0  0  0 20]]\n"]}]},{"cell_type":"markdown","source":["Q. 주어진 텍스트 데이터셋을 토큰화하고, 각 단어에 고유한 정수 인덱스를 할당하세요. 그런 다음, 각 문장을 정수 시퀀스로 변환하세요.\n","\n","데이터셋:\n","\n","\"I love machine learning.\"<br>\n","\"I love coding in Python.\"<br>\n","\"Machine learning can be fun.\""],"metadata":{"id":"ftw5z6bxDhO9"}},{"cell_type":"code","source":["from keras.preprocessing.text import Tokenizer\n","\n","# 데이터셋 정의\n","texts = [\"I love machine learning.\", \"I love coding in Python.\", \"Machine learning can be fun.\"]\n","\n","token = Tokenizer()\n","token.fit_on_texts(texts)\n","\n","word_index = token.word_index\n","print(\"단어 인덱스:\\n\", word_index)\n","\n","sequences = token.texts_to_sequences(texts)\n","\n","print(\"\\n텍스트를 정수 시퀀스로 변환:\\n\", sequences)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EdAtZLyGCVdu","executionInfo":{"status":"ok","timestamp":1707194041734,"user_tz":-540,"elapsed":515,"user":{"displayName":"이민지","userId":"02649806100014329289"}},"outputId":"9bdd22df-c36a-4560-855a-1e225668d695"},"execution_count":42,"outputs":[{"output_type":"stream","name":"stdout","text":["단어 인덱스:\n"," {'i': 1, 'love': 2, 'machine': 3, 'learning': 4, 'coding': 5, 'in': 6, 'python': 7, 'can': 8, 'be': 9, 'fun': 10}\n","\n","텍스트를 정수 시퀀스로 변환:\n"," [[1, 2, 3, 4], [1, 2, 5, 6, 7], [3, 4, 8, 9, 10]]\n"]}]},{"cell_type":"markdown","source":["Q. 위에서 생성한 정수 스퀀스에 패딩을 추가하여 모든 스퀀스의 길이를 동일하게 만드세요."],"metadata":{"id":"cGLEarDZPYNi"}},{"cell_type":"code","source":["from keras.preprocessing.sequence import pad_sequences\n","\n","# 패딩 추가\n","padded_sequences = pad_sequences(sequences, padding='post')\n","\n","# 패딩된 시퀀스 출력\n","print('패딩된 정수 시퀀스:', padded_sequences)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PyWx-vMkO3H3","executionInfo":{"status":"ok","timestamp":1707194042637,"user_tz":-540,"elapsed":516,"user":{"displayName":"이민지","userId":"02649806100014329289"}},"outputId":"2617b215-f45a-440d-d34a-1697c7179649"},"execution_count":43,"outputs":[{"output_type":"stream","name":"stdout","text":["패딩된 정수 시퀀스: [[ 1  2  3  4  0]\n"," [ 1  2  5  6  7]\n"," [ 3  4  8  9 10]]\n"]}]},{"cell_type":"markdown","source":["Q. 주어진 텍스트의 데이터셋에 대한 원-핫 인코딩을 수행하세요\n","- 사용할 최대 단어 수 설정: num_words=1000"],"metadata":{"id":"6Xa1YJT4Eocl"}},{"cell_type":"code","source":["# 첫 번째 열에 1이 나타나는 이유는 여기서 0 인덱스가 실제로 원-핫 인코딩에서 하나의 위치를 차지하고 있기 때문\n","# 토큰화 및 시퀀스 패딩 과정에서 0은 일반적으로 패딩을 위해 사용\n","from keras.preprocessing.text import Tokenizer\n","from keras.utils import to_categorical # to_categorical import 추가\n","\n","# 데이터 정의\n","texts = [\"I love machine learning.\", \"I love coding in Python.\", \"Machine learning can be fun.\"]\n","\n","# 토근화 객체 생성\n","tokenizer = Tokenizer(num_words=1000)\n","\n","# 데이터셋에 대해 토큰화 수행\n","tokenizer.fit_on_texts(texts) # 여기를 수정\n","\n","# 각 문장을 정수 시퀀스로 변환\n","sequences = tokenizer.texts_to_sequences(texts)\n","print(sequences)\n","\n","# 모든 시퀀스를 동일한 길이로 패딩\n","padded_sequences = pad_sequences(sequences, padding='post')\n","print(padded_sequences)\n","\n","word_size = len(tokenizer.word_index) + 1\n","x = to_categorical(padded_sequences, num_classes=word_size) # to_categorical 사용\n","\n","# 결과 출력\n","print('패딩된 시퀀스:',x)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qfJm9PD2FWdQ","executionInfo":{"status":"ok","timestamp":1707194043590,"user_tz":-540,"elapsed":2,"user":{"displayName":"이민지","userId":"02649806100014329289"}},"outputId":"7d407f8d-a195-4679-ee72-96bd13ebe4b2"},"execution_count":44,"outputs":[{"output_type":"stream","name":"stdout","text":["[[1, 2, 3, 4], [1, 2, 5, 6, 7], [3, 4, 8, 9, 10]]\n","[[ 1  2  3  4  0]\n"," [ 1  2  5  6  7]\n"," [ 3  4  8  9 10]]\n","패딩된 시퀀스: [[[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","  [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n","  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n","\n"," [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","  [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n","\n"," [[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n","  [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n","  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]]\n"]}]},{"cell_type":"markdown","source":["### Embedding\n","- 기계 학습, 특히 자연어 처리(NLP)의 맥락에서 임베딩은 단어, 구문 또는 기타 유형의 엔터티를 실수의 밀집된 벡터로 표현하는 기술을 의미. 핵심 아이디어는 이러한 엔터티의 의미론적 의미를 연속적인 벡터 공간으로 인코딩하는 것이다. 여기서 벡터 간의 기하학적 관계는 이들이 나타내는 엔터티 간의 의미론적 관계를 반영하며 이 접근 방식은 의미론적 관계가 캡처되지 않는 원-핫 인코딩과 같은 희소 표현과 대조된다.\n","- 의미:\n","  - 의미적 표현: 임베딩 벡터는 단어나 개체의 의미를 포착하도록 설계된다. 비슷한 의미를 가진 단어는 임베딩 공간에서 서로 가까운 벡터를 갖도록 처리된다.\n","  - 차원성 감소: 임베딩은 고차원 공간(예: 원-핫 인코딩된 벡터)의 단어를 저차원의 연속 벡터 공간으로 매핑하며 이는 표현을 더욱 효율적으로 만들고 계산 복잡성을 줄인다.\n","  - 컨텍스트화: 고급 임베딩 모델(예: Word2Vec, GloVe, BERT)에서는 단어가 나타나는 컨텍스트가 벡터 표현에 영향을 미치므로 모델이 다양한 컨텍스트에서 단어의 다양한 의미를 캡처할 수 있다.\n","- 임베딩 생성 방법:\n","  - 사전 훈련된 임베딩: 일반적인 접근 방식 중 하나는 대규모 텍스트 모음에 대해 사전 훈련된 임베딩을 사용하는 것으로 Word2Vec, GloVe와 같은 모델을 사용하면 훈련 코퍼스에서 풍부한 의미 체계 관계를 학습한 사전 훈련된 모델을 기반으로 단어를 벡터에 매핑할 수 있다.\n","    - Word2Vec: 컨텍스트를 사용하여 대상 단어를 예측하거나(CBOW) 단어를 사용하여 컨텍스트를 예측(Skip-gram)하여 임베딩을 학습한다.\n","      - CBOW 모델은 주어진 컨텍스트(주변 단어들)를 바탕으로 타겟 단어를 예측하는 방식으로 작동. 예를 들어, 문장 \"the cat sits on the\"에서 \"cat\", \"sits\", \"on\", \"the\"를 컨텍스트로 사용하여 \"mat\"이라는 타겟 단어를 예측\n","      - kip-gram 모델은 CBOW와 반대로, 주어진 타겟 단어로부터 컨텍스트(주변 단어들)를 예측하는 방식으로 작동. 예를 들어, \"cat\"이라는 단어가 주어졌을 때, \"the\", \"sits\", \"on\", \"the\"와 같은 주변 단어들을 예측\n","    - GloVe: 단어 동시 발생 통계 행렬을 인수분해하여 임베딩을 학습한다.    \n","  - 자신만의 임베딩 훈련: 신경망을 사용하여 처음부터 자신만의 임베딩을 훈련할 수도 있다."],"metadata":{"id":"pMrweABxRAQ4"}},{"cell_type":"markdown","source":["텍스트 입력을 표현하기 위해 단어 임베딩 활용\n","- model.add(Embedding(word_size, 8, input_length=4)): 이 줄은 모델에 임베딩 레이어를 추가. 임베딩 레이어는 단어 임베딩을 만드는 데 사용. 각 매개변수의 의미는 다음과 같다.\n","  - word_size: 입력 차원의 크기, 즉 어휘 크기. 데이터 세트에 있는 총 고유 단어 수에 1을 더한 값.\n","  - 8: 임베딩 벡터의 크기. 각 단어는 8차원 벡터로 표현.\n","  - input_length=4: 입력 시퀀스의 길이. 이 모델은 각 입력 시퀀스의 길이가 4일 것으로 예상(예: 입력당 4개의 단어).\n","- model.add(Flatten()): 임베딩 레이어 이후 출력 모양은 배치 크기를 포함하여 3차원. 이 레이어는 출력을 2차원(배치 크기, input_length * 8)으로 평면화하여 밀도가 높은 레이어에 직접 공급될 수 있도록 한다."],"metadata":{"id":"VJy92yNVUBu5"}},{"cell_type":"code","source":["# 텍스트 리뷰 자료를 지정합니다.\n","docs = [\"너무 재밌네요\",\"최고예요\",\"참 잘 만든 영화예요\",\"추천하고 싶은 영화입니다\",\"한번 더 보고싶네요\",\"글쎄요\",\"별로예요\",\"생각보다 지루하네요\",\"연기가 어색해요\",\"재미없어요\"]\n","\n","# 긍정 리뷰는 1, 부정 리뷰는 0으로 클래스를 지정합니다.\n","classes = array([1,1,1,1,1,0,0,0,0,0])"],"metadata":{"id":"DGevqO2lpbsf","executionInfo":{"status":"ok","timestamp":1707194046151,"user_tz":-540,"elapsed":343,"user":{"displayName":"이민지","userId":"02649806100014329289"}}},"execution_count":45,"outputs":[]},{"cell_type":"code","source":["from keras.preprocessing.text import Tokenizer\n","token = Tokenizer()\n","token.fit_on_texts(docs)\n","print(token.word_index)\n","\n","# 임베딩에 입력될 단어의 수를 지정합니다.\n","word_size = len(token.word_index) +1\n","\n","# 단어 임베딩을 포함하여 딥러닝 모델을 만들고 결과를 출력합니다.\n","model = Sequential()\n","model.add(Embedding(word_size, 8, input_length=4))\n","model.add(Flatten())\n","model.add(Dense(1, activation='sigmoid'))\n","model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kxxhkRhDQi1y","executionInfo":{"status":"ok","timestamp":1707194047486,"user_tz":-540,"elapsed":3,"user":{"displayName":"이민지","userId":"02649806100014329289"}},"outputId":"accce8d2-bc48-45f5-811a-4f6bc6f30b5c"},"execution_count":46,"outputs":[{"output_type":"stream","name":"stdout","text":["{'너무': 1, '재밌네요': 2, '최고예요': 3, '참': 4, '잘': 5, '만든': 6, '영화예요': 7, '추천하고': 8, '싶은': 9, '영화입니다': 10, '한번': 11, '더': 12, '보고싶네요': 13, '글쎄요': 14, '별로예요': 15, '생각보다': 16, '지루하네요': 17, '연기가': 18, '어색해요': 19, '재미없어요': 20}\n","Model: \"sequential_8\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding_8 (Embedding)     (None, 4, 8)              168       \n","                                                                 \n"," flatten_8 (Flatten)         (None, 32)                0         \n","                                                                 \n"," dense_8 (Dense)             (None, 1)                 33        \n","                                                                 \n","=================================================================\n","Total params: 201 (804.00 Byte)\n","Trainable params: 201 (804.00 Byte)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]}]},{"cell_type":"code","source":["model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","model.fit(padded_x, classes, epochs=20)\n","print('\\n Accuracy: %.4f' % (model.evaluate(padded_x, classes)[1]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gNaD482fo-Ia","executionInfo":{"status":"ok","timestamp":1707194051916,"user_tz":-540,"elapsed":3109,"user":{"displayName":"이민지","userId":"02649806100014329289"}},"outputId":"728b22b3-bd34-4eaf-8cbf-40a5bd42810d"},"execution_count":47,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/20\n","1/1 [==============================] - 2s 2s/step - loss: 0.6882 - accuracy: 0.6000\n","Epoch 2/20\n","1/1 [==============================] - 0s 10ms/step - loss: 0.6863 - accuracy: 0.6000\n","Epoch 3/20\n","1/1 [==============================] - 0s 11ms/step - loss: 0.6844 - accuracy: 0.6000\n","Epoch 4/20\n","1/1 [==============================] - 0s 10ms/step - loss: 0.6825 - accuracy: 0.8000\n","Epoch 5/20\n","1/1 [==============================] - 0s 10ms/step - loss: 0.6806 - accuracy: 0.9000\n","Epoch 6/20\n","1/1 [==============================] - 0s 9ms/step - loss: 0.6787 - accuracy: 1.0000\n","Epoch 7/20\n","1/1 [==============================] - 0s 10ms/step - loss: 0.6767 - accuracy: 1.0000\n","Epoch 8/20\n","1/1 [==============================] - 0s 12ms/step - loss: 0.6748 - accuracy: 1.0000\n","Epoch 9/20\n","1/1 [==============================] - 0s 10ms/step - loss: 0.6729 - accuracy: 1.0000\n","Epoch 10/20\n","1/1 [==============================] - 0s 11ms/step - loss: 0.6710 - accuracy: 1.0000\n","Epoch 11/20\n","1/1 [==============================] - 0s 10ms/step - loss: 0.6690 - accuracy: 1.0000\n","Epoch 12/20\n","1/1 [==============================] - 0s 9ms/step - loss: 0.6671 - accuracy: 1.0000\n","Epoch 13/20\n","1/1 [==============================] - 0s 9ms/step - loss: 0.6651 - accuracy: 1.0000\n","Epoch 14/20\n","1/1 [==============================] - 0s 9ms/step - loss: 0.6632 - accuracy: 1.0000\n","Epoch 15/20\n","1/1 [==============================] - 0s 11ms/step - loss: 0.6612 - accuracy: 1.0000\n","Epoch 16/20\n","1/1 [==============================] - 0s 9ms/step - loss: 0.6592 - accuracy: 1.0000\n","Epoch 17/20\n","1/1 [==============================] - 0s 10ms/step - loss: 0.6572 - accuracy: 1.0000\n","Epoch 18/20\n","1/1 [==============================] - 0s 9ms/step - loss: 0.6552 - accuracy: 1.0000\n","Epoch 19/20\n","1/1 [==============================] - 0s 9ms/step - loss: 0.6532 - accuracy: 1.0000\n","Epoch 20/20\n","1/1 [==============================] - 0s 9ms/step - loss: 0.6512 - accuracy: 1.0000\n","1/1 [==============================] - 0s 138ms/step - loss: 0.6491 - accuracy: 1.0000\n","\n"," Accuracy: 1.0000\n"]}]},{"cell_type":"code","source":["x = token.texts_to_sequences(docs)\n","padded_x = pad_sequences(x,padding='post')"],"metadata":{"id":"0iqebSxRsgWs","executionInfo":{"status":"ok","timestamp":1707194492157,"user_tz":-540,"elapsed":2,"user":{"displayName":"이민지","userId":"02649806100014329289"}}},"execution_count":48,"outputs":[]},{"cell_type":"code","source":["model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","model.fit(padded_x, classes, epochs=20)\n","print('\\n Accuracy: %.4f' % (model.evaluate(padded_x, classes)[1]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8Skp_VpKvcuf","executionInfo":{"status":"ok","timestamp":1707194836194,"user_tz":-540,"elapsed":2267,"user":{"displayName":"이민지","userId":"02649806100014329289"}},"outputId":"7df258e6-89d4-43ab-dc23-2cc94a5869fd"},"execution_count":49,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/20\n","1/1 [==============================] - 1s 953ms/step - loss: 0.6777 - accuracy: 0.6000\n","Epoch 2/20\n","1/1 [==============================] - 0s 12ms/step - loss: 0.6756 - accuracy: 0.7000\n","Epoch 3/20\n","1/1 [==============================] - 0s 13ms/step - loss: 0.6736 - accuracy: 0.8000\n","Epoch 4/20\n","1/1 [==============================] - 0s 10ms/step - loss: 0.6715 - accuracy: 0.8000\n","Epoch 5/20\n","1/1 [==============================] - 0s 11ms/step - loss: 0.6694 - accuracy: 0.8000\n","Epoch 6/20\n","1/1 [==============================] - 0s 10ms/step - loss: 0.6673 - accuracy: 0.8000\n","Epoch 7/20\n","1/1 [==============================] - 0s 10ms/step - loss: 0.6652 - accuracy: 0.8000\n","Epoch 8/20\n","1/1 [==============================] - 0s 10ms/step - loss: 0.6631 - accuracy: 0.8000\n","Epoch 9/20\n","1/1 [==============================] - 0s 10ms/step - loss: 0.6610 - accuracy: 0.8000\n","Epoch 10/20\n","1/1 [==============================] - 0s 11ms/step - loss: 0.6589 - accuracy: 0.8000\n","Epoch 11/20\n","1/1 [==============================] - 0s 10ms/step - loss: 0.6568 - accuracy: 0.8000\n","Epoch 12/20\n","1/1 [==============================] - 0s 13ms/step - loss: 0.6546 - accuracy: 0.8000\n","Epoch 13/20\n","1/1 [==============================] - 0s 12ms/step - loss: 0.6525 - accuracy: 0.8000\n","Epoch 14/20\n","1/1 [==============================] - 0s 11ms/step - loss: 0.6504 - accuracy: 0.8000\n","Epoch 15/20\n","1/1 [==============================] - 0s 11ms/step - loss: 0.6482 - accuracy: 0.8000\n","Epoch 16/20\n","1/1 [==============================] - 0s 12ms/step - loss: 0.6461 - accuracy: 0.8000\n","Epoch 17/20\n","1/1 [==============================] - 0s 12ms/step - loss: 0.6439 - accuracy: 0.8000\n","Epoch 18/20\n","1/1 [==============================] - 0s 11ms/step - loss: 0.6418 - accuracy: 0.8000\n","Epoch 19/20\n","1/1 [==============================] - 0s 11ms/step - loss: 0.6396 - accuracy: 0.8000\n","Epoch 20/20\n","1/1 [==============================] - 0s 13ms/step - loss: 0.6374 - accuracy: 0.8000\n","1/1 [==============================] - 0s 175ms/step - loss: 0.6352 - accuracy: 0.8000\n","\n"," Accuracy: 0.8000\n"]}]},{"cell_type":"markdown","source":["Q. 10개의 단어를 가진 어휘(vocabulary)와 각 단어를 4차원 벡터로 임베딩하는 Keras 모델을 구성하세요. 시퀀스의 최대 길이는 5로 설정하세요."],"metadata":{"id":"aY6iL_CguK9X"}},{"cell_type":"code","source":["from keras.models import Sequential\n","from keras.layers import Embedding\n","\n","# 어휘 크기 설정\n","vocab_size = 10\n","\n","model = Sequential()\n","model.add(Embedding(input_dim=10, output_dim = 4, input_length=5))\n","model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vQG3GJzSuSVu","executionInfo":{"status":"ok","timestamp":1707196224841,"user_tz":-540,"elapsed":349,"user":{"displayName":"이민지","userId":"02649806100014329289"}},"outputId":"35fa99a4-7287-4a45-9608-1536a370b268"},"execution_count":50,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_9\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding_9 (Embedding)     (None, 5, 4)              40        \n","                                                                 \n","=================================================================\n","Total params: 40 (160.00 Byte)\n","Trainable params: 40 (160.00 Byte)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]}]},{"cell_type":"markdown","source":["Q. 100개의 단어를 가진 어휘와 각 단어를 8차원 벡터로 임베딩한 후, 이를 평탄화하고 Dense 레이어를 통해 분류하는 Keras 모델을 구성하세요. 최종 출력은 2개의 클래스를 가진 분류 문제를 가정"],"metadata":{"id":"en0aDyhGuNxC"}},{"cell_type":"code","source":["from keras.models import Sequential\n","from keras.layers import Embedding\n","\n","# 모델 구성\n","model = Sequential()\n","model.add(Embedding(input_dim=100, output_dim=8, input_length=10))\n","model.add(Flatten())\n","model.add(Dense(3, activation='softmax'))\n","\n","# 모델 요약 출력\n","model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bQ5YMIF8uRm3","executionInfo":{"status":"ok","timestamp":1707196467003,"user_tz":-540,"elapsed":4,"user":{"displayName":"이민지","userId":"02649806100014329289"}},"outputId":"b728b0a1-cdef-4e74-f2c1-c705c6ee3635"},"execution_count":51,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_10\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding_10 (Embedding)    (None, 10, 8)             800       \n","                                                                 \n"," flatten_9 (Flatten)         (None, 80)                0         \n","                                                                 \n"," dense_9 (Dense)             (None, 3)                 243       \n","                                                                 \n","=================================================================\n","Total params: 1043 (4.07 KB)\n","Trainable params: 1043 (4.07 KB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]}]},{"cell_type":"markdown","source":["Q. 간단한 분류 모델을 구성하고, 이를 컴파일하는 코드를 작성하세요. 이 모델은 이진 분류 문제를 해결하며, 다음 조건을 만족해야 한다:\n","\n","- 입력 차원은 20.\n","- 첫 번째 Dense 레이어는 64 유닛과 ReLU 활성화 함수를 사용.\n","- 출력 레이어는 1 유닛과 시그모이드 활성화 함수를 사용.\n","- 손실 함수로는 binary crossentropy를 사용.\n","- 옵티마이저로는 'adam'을 사용.\n","- 평가 지표로는 'accuracy'를 사용."],"metadata":{"id":"FOiVHrBtuPgo"}},{"cell_type":"code","source":["from keras.models import Sequential\n","from keras.layers import Dense\n","\n","# 모델 구성\n","model.compile(optimizer='adam',\n","              loss ='binary_crossentropy',\n","              metrics=['accuracy'])\n","\n","# 모델 요약 출력\n","model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"15i8DeVO1t91","executionInfo":{"status":"ok","timestamp":1707196726412,"user_tz":-540,"elapsed":3,"user":{"displayName":"이민지","userId":"02649806100014329289"}},"outputId":"c9dfa1a1-05c2-4f49-dd56-977ef324082d"},"execution_count":52,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_10\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding_10 (Embedding)    (None, 10, 8)             800       \n","                                                                 \n"," flatten_9 (Flatten)         (None, 80)                0         \n","                                                                 \n"," dense_9 (Dense)             (None, 3)                 243       \n","                                                                 \n","=================================================================\n","Total params: 1043 (4.07 KB)\n","Trainable params: 1043 (4.07 KB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]}]},{"cell_type":"markdown","source":["Q1_0206. IMDB 영화 리뷰 데이터셋을 사용하여 긍부정 이진분류 모델링 및 평가를 수행하세요. 단, embedding 차원은 8"],"metadata":{"id":"DXGwUcA-5Rfj"}},{"cell_type":"code","source":["# Q7_0206. IMDB 영화 리뷰 데이터셋을 사용하여 긍/부정 이진분류 모델링 및 평가를 수행하세요. Embedding 차원 8\n","from keras.datasets import imdb\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.models import Sequential\n","from keras.layers import Embedding, Flatten, Dense\n","from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n","\n","# 데이터셋 로드\n","(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words = 10000)\n","\n","# 시퀀스 데이터 패딩\n","x_train = pad_sequences(x_train, maxlen = 100)\n","x_test = pad_sequences(x_test, maxlen = 100)\n","\n","# 모델 구축\n","word_size = 10000\n","model = Sequential([\n","    Embedding(word_size, 8, input_length = 100),\n","    Flatten(),\n","    Dense(10, activation = 'relu'),\n","    Dense(1, activation='sigmoid')  # 이진분류는 출력 뉴런 수를 1로 설정\n","])\n","\n","# 모델 요약 출력\n","model.summary()\n","\n","# EarlyStopping 설정\n","Early_Stopping_Callbacks = EarlyStopping(monitor = 'val_accuracy', patience = 20)\n","\n","# 모델 컴파일 및 결과 출력\n","model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n","model.fit(x_train, y_train, epochs = 20000, batch_size = 20, verbose = 1, validation_split = 0.25, callbacks = [Early_Stopping_Callbacks])\n","print('\\nAccuracy: %.4f' % model.evaluate(x_test, y_test)[1])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nLALaMVe4J2n","executionInfo":{"status":"ok","timestamp":1707200988238,"user_tz":-540,"elapsed":162792,"user":{"displayName":"이민지","userId":"02649806100014329289"}},"outputId":"36c844b2-3992-49b7-d8d4-2b8e8ed67a81"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n","17464789/17464789 [==============================] - 0s 0us/step\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       (None, 100, 8)            80000     \n","                                                                 \n"," flatten (Flatten)           (None, 800)               0         \n","                                                                 \n"," dense (Dense)               (None, 10)                8010      \n","                                                                 \n"," dense_1 (Dense)             (None, 1)                 11        \n","                                                                 \n","=================================================================\n","Total params: 88021 (343.83 KB)\n","Trainable params: 88021 (343.83 KB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","Epoch 1/20000\n","938/938 [==============================] - 5s 4ms/step - loss: 0.5052 - accuracy: 0.7299 - val_loss: 0.3536 - val_accuracy: 0.8394\n","Epoch 2/20000\n","938/938 [==============================] - 7s 8ms/step - loss: 0.2314 - accuracy: 0.9105 - val_loss: 0.3634 - val_accuracy: 0.8411\n","Epoch 3/20000\n","938/938 [==============================] - 5s 6ms/step - loss: 0.0965 - accuracy: 0.9724 - val_loss: 0.4545 - val_accuracy: 0.8315\n","Epoch 4/20000\n","938/938 [==============================] - 4s 5ms/step - loss: 0.0280 - accuracy: 0.9959 - val_loss: 0.5485 - val_accuracy: 0.8288\n","Epoch 5/20000\n","938/938 [==============================] - 3s 4ms/step - loss: 0.0081 - accuracy: 0.9992 - val_loss: 0.6321 - val_accuracy: 0.8232\n","Epoch 6/20000\n","938/938 [==============================] - 4s 4ms/step - loss: 0.0023 - accuracy: 0.9999 - val_loss: 0.6991 - val_accuracy: 0.8269\n","Epoch 7/20000\n","938/938 [==============================] - 4s 5ms/step - loss: 8.0523e-04 - accuracy: 1.0000 - val_loss: 0.7516 - val_accuracy: 0.8275\n","Epoch 8/20000\n","938/938 [==============================] - 3s 3ms/step - loss: 3.8804e-04 - accuracy: 1.0000 - val_loss: 0.8022 - val_accuracy: 0.8269\n","Epoch 9/20000\n","938/938 [==============================] - 3s 3ms/step - loss: 2.1382e-04 - accuracy: 1.0000 - val_loss: 0.8490 - val_accuracy: 0.8283\n","Epoch 10/20000\n","938/938 [==============================] - 3s 3ms/step - loss: 1.1971e-04 - accuracy: 1.0000 - val_loss: 0.8949 - val_accuracy: 0.8275\n","Epoch 11/20000\n","938/938 [==============================] - 4s 4ms/step - loss: 6.8338e-05 - accuracy: 1.0000 - val_loss: 0.9430 - val_accuracy: 0.8272\n","Epoch 12/20000\n","938/938 [==============================] - 3s 3ms/step - loss: 3.9221e-05 - accuracy: 1.0000 - val_loss: 0.9887 - val_accuracy: 0.8270\n","Epoch 13/20000\n","938/938 [==============================] - 3s 3ms/step - loss: 2.2697e-05 - accuracy: 1.0000 - val_loss: 1.0371 - val_accuracy: 0.8278\n","Epoch 14/20000\n","938/938 [==============================] - 3s 4ms/step - loss: 1.3013e-05 - accuracy: 1.0000 - val_loss: 1.0823 - val_accuracy: 0.8267\n","Epoch 15/20000\n","938/938 [==============================] - 3s 4ms/step - loss: 7.5809e-06 - accuracy: 1.0000 - val_loss: 1.1302 - val_accuracy: 0.8283\n","Epoch 16/20000\n","938/938 [==============================] - 3s 3ms/step - loss: 4.4589e-06 - accuracy: 1.0000 - val_loss: 1.1727 - val_accuracy: 0.8270\n","Epoch 17/20000\n","938/938 [==============================] - 3s 3ms/step - loss: 2.6343e-06 - accuracy: 1.0000 - val_loss: 1.2193 - val_accuracy: 0.8267\n","Epoch 18/20000\n","938/938 [==============================] - 5s 5ms/step - loss: 1.5527e-06 - accuracy: 1.0000 - val_loss: 1.2635 - val_accuracy: 0.8269\n","Epoch 19/20000\n","938/938 [==============================] - 4s 4ms/step - loss: 9.3314e-07 - accuracy: 1.0000 - val_loss: 1.3086 - val_accuracy: 0.8272\n","Epoch 20/20000\n","938/938 [==============================] - 3s 4ms/step - loss: 5.6489e-07 - accuracy: 1.0000 - val_loss: 1.3537 - val_accuracy: 0.8277\n","Epoch 21/20000\n","938/938 [==============================] - 4s 4ms/step - loss: 3.2894e-07 - accuracy: 1.0000 - val_loss: 1.3971 - val_accuracy: 0.8267\n","Epoch 22/20000\n","938/938 [==============================] - 4s 5ms/step - loss: 2.0229e-07 - accuracy: 1.0000 - val_loss: 1.4389 - val_accuracy: 0.8269\n","782/782 [==============================] - 1s 2ms/step - loss: 1.4207 - accuracy: 0.8271\n","\n","Accuracy: 0.8271\n"]}]}]}